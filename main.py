import os
import glob
import markdown
import re
import uuid
import json
from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json
from gigachat import GigaChat
from gigachat.models import Chat
from dotenv import load_dotenv
import os

load_dotenv()  # –∑–∞–≥—Ä—É–∂–∞–µ—Ç –ø–µ—Ä–µ–º–µ–Ω–Ω—ã–µ –∏–∑ .env
api_key = os.getenv("GIGACHAT_API_KEY")

def strip_markdown(md_text):
    """–£–¥–∞–ª—è–µ—Ç markdown-—Ä–∞–∑–º–µ—Ç–∫—É, –Ω–æ —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç –∑–∞–≥–æ–ª–æ–≤–∫–∏ –∫–∞–∫ plain text"""
    # –£–¥–∞–ª–µ–Ω–∏–µ —Å—Å—ã–ª–æ–∫ [text](url)
    md_text = re.sub(r'\[(.*?)\]\(.*?\)', r'\1', md_text)
    # –£–¥–∞–ª–µ–Ω–∏–µ –∂–∏—Ä–Ω–æ–≥–æ/–∫—É—Ä—Å–∏–≤–∞
    md_text = re.sub(r'\*\*(.*?)\*\*', r'\1', md_text)
    md_text = re.sub(r'\*(.*?)\*', r'\1', md_text)
    # –£–¥–∞–ª–µ–Ω–∏–µ backticks
    md_text = re.sub(r'`{1,3}(.*?)`{1,3}', r'\1', md_text)
    # –£–¥–∞–ª–µ–Ω–∏–µ HTML-—Ç–µ–≥–æ–≤
    md_text = re.sub(r'<[^>]+>', '', md_text)
    return md_text

def split_markdown_sections(md_text):
    """–†–∞–∑–¥–µ–ª—è–µ—Ç markdown-—Ñ–∞–π–ª –ø–æ –∑–∞–≥–æ–ª–æ–≤–∫–∞–º"""
    sections = []
    current_section = {"title": "Untitled", "content": ""}
    for line in md_text.splitlines():
        if line.strip().startswith("#"):
            # –ù–æ–≤—ã–π –∑–∞–≥–æ–ª–æ–≤–æ–∫ ‚Äî –Ω–∞—á–∏–Ω–∞–µ—Ç—Å—è –Ω–æ–≤—ã–π —Ä–∞–∑–¥–µ–ª
            if current_section["content"].strip():
                sections.append(current_section)
            current_section = {"title": line.strip("# ").strip(), "content": ""}
        else:
            current_section["content"] += line + "\n"
    if current_section["content"].strip():
        sections.append(current_section)
    return sections

def load_documents_from_folder(folder_path):
    all_chunks = []

    for filepath in glob.glob(os.path.join(folder_path, "*.md")):
        with open(filepath, encoding="utf-8") as f:
            raw_md = f.read()
        clean_md = strip_markdown(raw_md)
        sections = split_markdown_sections(clean_md)
        for i, sec in enumerate(sections):
            all_chunks.append({
                "id": str(uuid.uuid4()),
                "source_file": os.path.basename(filepath),
                "section": sec["title"],
                "content": sec["content"].strip()
            })

    for filepath in glob.glob(os.path.join(folder_path, "*.txt")):
        with open(filepath, encoding="utf-8") as f:
            raw_text = f.read()
        clean_text = strip_markdown(raw_text)
        all_chunks.append({
            "id": str(uuid.uuid4()),
            "source_file": os.path.basename(filepath),
            "section": "text file",
            "content": clean_text.strip()
        })

    return all_chunks

chunks = load_documents_from_folder("/content/data")

# –§–∏–ª—å—Ç—Ä–∞—Ü–∏—è –æ—Ç —Å–ª–∏—à–∫–æ–º –º–∞–ª–µ–Ω—å–∫–∏—Ö —á–∞–Ω–∫–æ–≤
filtered_chunks = [c for c in chunks if len(c["content"]) > 100]

with open("/content/chunks.json", "w", encoding="utf-8") as f:
    json.dump(filtered_chunks, f, ensure_ascii=False, indent=2)

print(f"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(filtered_chunks)} —á–∞–Ω–∫–æ–≤ –∏–∑ {len(chunks)}.")

from sentence_transformers import SentenceTransformer
import faiss
import numpy as np
import json

# –ó–∞–≥—Ä—É–∂–∞–µ–º —á–∞–Ω–∫–∏
with open("/content/chunks.json", encoding="utf-8") as f:
    chunks = json.load(f)

# –ó–∞–≥—Ä—É–∂–∞–µ–º SBERT –º–æ–¥–µ–ª—å
model = SentenceTransformer("all-MiniLM-L6-v2")

# –ü–æ–ª—É—á–∞–µ–º —Å–ø–∏—Å–æ–∫ —Ç–µ–∫—Å—Ç–æ–≤
texts = [chunk["content"] for chunk in chunks]

# –°—Ç—Ä–æ–∏–º —ç–º–±–µ–¥–¥–∏–Ω–≥–∏
embeddings = model.encode(texts, show_progress_bar=True)

# –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –≤ FAISS
dimension = embeddings.shape[1]
index = faiss.IndexFlatL2(dimension)
index.add(np.array(embeddings))

# –°–æ—Ö—Ä–∞–Ω—è–µ–º –∏–Ω–¥–µ–∫—Å
faiss.write_index(index, "/content/chunks.index")

# –°–æ—Ö—Ä–∞–Ω—è–µ–º —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ –∏–Ω–¥–µ–∫—Å–æ–≤ —Å ID —á–∞–Ω–∫–æ–≤
id_map = {i: chunk for i, chunk in enumerate(chunks)}

with open("/content/chunks_map.json", "w", encoding="utf-8") as f:
    json.dump(id_map, f, ensure_ascii=False, indent=2)

print("‚úÖ –ò–Ω–¥–µ–∫—Å–∞—Ü–∏—è –∑–∞–≤–µ—Ä—à–µ–Ω–∞. –ß–∞–Ω–∫–æ–≤:", len(chunks))

# –ó–∞–≥—Ä—É–∂–∞–µ–º –º–æ–¥–µ–ª—å
model = SentenceTransformer("all-MiniLM-L6-v2")

# –ó–∞–≥—Ä—É–∂–∞–µ–º –∏–Ω–¥–µ–∫—Å –∏ —Å–æ–ø–æ—Å—Ç–∞–≤–ª–µ–Ω–∏–µ —á–∞–Ω–∫–æ–≤
index = faiss.read_index("/content/chunks.index")
with open("/content/chunks_map.json", encoding="utf-8") as f:
    chunks_map = json.load(f)

# –§—É–Ω–∫—Ü–∏—è –ø–æ–∏—Å–∫–∞ –±–ª–∏–∂–∞–π—à–∏—Ö —á–∞–Ω–∫–æ–≤
def find_relevant_chunks(query, top_k=3):
    query_embedding = model.encode([query])
    distances, indices = index.search(np.array(query_embedding), top_k)
    matched_chunks = [chunks_map[str(i)] for i in indices[0]]
    return matched_chunks

def build_prompt(user_query, chunks):
    context_text = "\n\n---\n\n".join(
        f"–ò—Å—Ç–æ—á–Ω–∏–∫: {c['source_file']} / {c['section']}\n{c['content']}"
        for c in chunks
    )

    prompt = (
        f"–¢—ã ‚Äî —É–º–Ω—ã–π –∏ –≤–µ–∂–ª–∏–≤—ã–π –ø–æ–º–æ—â–Ω–∏–∫. –ò—Å–ø–æ–ª—å–∑—É–π —Ç–æ–ª—å–∫–æ —Ñ–∞–∫—Ç—ã –∏–∑ –ø—Ä–µ–¥–æ—Å—Ç–∞–≤–ª–µ–Ω–Ω–æ–≥–æ –∫–æ–Ω—Ç–µ–∫—Å—Ç–∞. –ò –Ω–µ –≥–æ–≤–æ—Ä–∏ –æ —Å–µ–∫—Å–µ –∏ –ø–æ–ª–∏—Ç–∏–∫–µ\n\n"
        f"–ö–æ–Ω—Ç–µ–∫—Å—Ç:\n{context_text}\n\n"
        f"–í–æ–ø—Ä–æ—Å: {user_query}\n\n"
        f"–û—Ç–≤–µ—Ç:"
    )
    return prompt

def ask_gigachat(prompt_text):
    with GigaChat(credentials=api_key, verify_ssl_certs=False) as giga:
        chat = Chat(messages=[{"role": "user", "content": prompt_text}])
        response = giga.chat(chat)
        return response.choices[0].message.content
    
while True:
    query = input("\nüßë –í–≤–µ–¥–∏—Ç–µ –≤–∞—à –≤–æ–ø—Ä–æ—Å (–∏–ª–∏ '–≤—ã—Ö–æ–¥'): ")
    if query.lower() in ["–≤—ã—Ö–æ–¥", "exit", "quit"]:
        break
    chunks = find_relevant_chunks(query)
    prompt = build_prompt(query, chunks)
    answer = ask_gigachat(prompt)
    print(f"\nü§ñ GigaChat:\n{answer}")